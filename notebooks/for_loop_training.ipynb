{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Model, model_from_json, load_model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Reshape, Bidirectional, LSTM, Dense, Lambda, Activation, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('preprocessed_train.csv')\n",
    "valid = pd.read_csv('preprocessed_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 4096\n",
    "valid_size= 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = np.arange(len(train))\n",
    "num_valid = np.arange(len(valid))\n",
    "np.random.shuffle(num_train)\n",
    "np.random.shuffle(num_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(idx, batch_size):\n",
    "    x = []\n",
    "    steps_done = idx * batch_size\n",
    "    indexes = num_train[steps_done : (idx + 1) * batch_size]\n",
    "    for i in indexes:\n",
    "        img_dir = 'processed_train/' + train.loc[i, 'FILENAME']\n",
    "        image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n",
    "        x.append(image)\n",
    "    x = np.array(x).reshape(-1, 256, 64, 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid(idx, batch_size):\n",
    "    x = []\n",
    "    steps_done = idx * batch_size\n",
    "    indexes = num_valid[steps_done : (idx + 1) * batch_size]\n",
    "    for i in indexes:\n",
    "        img_dir = 'processed_valid/' + valid.loc[i, 'FILENAME']\n",
    "        image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n",
    "        x.append(image)\n",
    "    x = np.array(x).reshape(-1, 256, 64, 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = u\"ABCDEFGHIJKLMNOPQRSTUVWXYZ-'` \"\n",
    "max_str_len = 34\n",
    "num_of_characters = len(alphabets) + 1\n",
    "num_of_timestamps = 64\n",
    "\n",
    "def label_to_num(label):\n",
    "    label_num = []\n",
    "    for ch in label:\n",
    "        label_num.append(alphabets.find(ch))\n",
    "        \n",
    "    return np.array(label_num)\n",
    "\n",
    "def num_to_label(num):\n",
    "    ret = \"\"\n",
    "    for ch in num:\n",
    "        if ch == -1:  # CTC Blank\n",
    "            break\n",
    "        else:\n",
    "            ret+=alphabets[ch]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_labels(idx, batch_size):\n",
    "    steps_done = idx * batch_size\n",
    "    indexes = num_train[steps_done : (idx + 1) * batch_size]\n",
    "\n",
    "    y = np.ones([batch_size, max_str_len]) * -1\n",
    "    label_len = np.zeros([batch_size, 1])\n",
    "    input_len = np.ones([batch_size, 1]) * (num_of_timestamps-2)\n",
    "    output = np.zeros([batch_size])\n",
    "\n",
    "    for index, i in enumerate(indexes):\n",
    "        label_len[index] = len(train.loc[i, 'IDENTITY'])\n",
    "        y[index, 0:len(train.loc[i, 'IDENTITY'])]= label_to_num(train.loc[i, 'IDENTITY'])\n",
    "\n",
    "    return y, label_len, input_len, output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_labels(idx, batch_size):\n",
    "    steps_done = idx * batch_size\n",
    "    indexes = num_valid[steps_done : (idx + 1) * batch_size]\n",
    "\n",
    "    y = np.ones([batch_size, max_str_len]) * -1\n",
    "    label_len = np.zeros([batch_size, 1])\n",
    "    input_len = np.ones([batch_size, 1]) * (num_of_timestamps-2)\n",
    "    output = np.zeros([batch_size])\n",
    "\n",
    "    for index, i in enumerate(indexes):\n",
    "        label_len[index] = len(valid.loc[i, 'IDENTITY'])\n",
    "        y[index, 0:len(valid.loc[i, 'IDENTITY'])]= label_to_num(valid.loc[i, 'IDENTITY'])\n",
    "\n",
    "    return y, label_len, input_len, output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Input(shape=(256, 64, 1), name='input')\n",
    "\n",
    "inner = Conv2D(32, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(input_data)  \n",
    "inner = BatchNormalization()(inner)\n",
    "inner = Activation('relu')(inner)\n",
    "inner = MaxPooling2D(pool_size=(2, 2), name='max1')(inner)\n",
    "\n",
    "inner = Conv2D(64, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(inner)\n",
    "inner = BatchNormalization()(inner)\n",
    "inner = Activation('relu')(inner)\n",
    "inner = MaxPooling2D(pool_size=(2, 2), name='max2')(inner)\n",
    "inner = Dropout(0.3)(inner)\n",
    "\n",
    "inner = Conv2D(128, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(inner)\n",
    "inner = BatchNormalization()(inner)\n",
    "inner = Activation('relu')(inner)\n",
    "inner = MaxPooling2D(pool_size=(1, 2), name='max3')(inner)\n",
    "inner = Dropout(0.3)(inner)\n",
    "\n",
    "inner = Reshape(target_shape=((64, 1024)), name='reshape')(inner)\n",
    "inner = Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(inner)\n",
    "\n",
    "inner = Bidirectional(LSTM(256, return_sequences=True, name='lstminner1'), name = 'lstm1')(inner) # try gru instead of lstm\n",
    "inner = Bidirectional(LSTM(256, return_sequences=True, name='lstminner2'), name = 'lstm2')(inner)\n",
    "\n",
    "inner = Dense(num_of_characters, kernel_initializer='he_normal',name='dense2')(inner)\n",
    "y_pred = Activation('softmax', name='softmax')(inner)\n",
    "\n",
    "model = Model(inputs=input_data, outputs=y_pred)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = Input(name='gtruth_labels', shape=[max_str_len], dtype='float32')\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "ctc_loss = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "model_final = Model(inputs=[input_data, labels, input_length, label_length], outputs=ctc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "model_final.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "mc = ModelCheckpoint('model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "mc2 = ModelCheckpoint('model_loss.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'optimizer'\n",
    "num_iterations = int(np.floor(len(train) / train_size))\n",
    "idx = 0\n",
    "for i in range(num_iterations):\n",
    "    train_x = get_train(idx, train_size)\n",
    "    valid_x = get_valid(idx, valid_size)\n",
    "    train_y, train_label_len, train_input_len, train_output = train_labels(idx, train_size)\n",
    "    valid_y, valid_label_len, valid_input_len, valid_output = valid_labels(idx, valid_size)\n",
    "\n",
    "    myModel = model_final.fit(x=[train_x, train_y, train_input_len, train_label_len], y=train_output, \n",
    "                validation_data=([valid_x, valid_y, valid_input_len, valid_label_len], valid_output),\n",
    "                epochs=120, shuffle=True, callbacks=[es, mc, mc2])\n",
    "    \n",
    "    np.save(f'{path}/optimizer.npy', optimizer.get_weights())\n",
    "\n",
    "    print(\"Saved model to disk, idx:\", idx)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading optimizer\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# # Load the optimizer weights\n",
    "# opt_weights = np.load(f'{path}/optimizer.npy', allow_pickle=True)\n",
    "\n",
    "# # Train a dummy record\n",
    "# # I'm using the universal sentence encoder which requires a string as input\n",
    "# with tf.GradientTape() as tape:\n",
    "#     # preduct a dummy record\n",
    "#     tmp = model('')\n",
    "#     # create a dummy loss\n",
    "#     loss = tf.reduce_mean((tmp - tmp)**2)\n",
    "\n",
    "# # calculate the gradiens and add the gradients\n",
    "# # the gradients should be near 0\n",
    "# gradients = tape.gradient(loss, model.trainable_variables)\n",
    "# optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "# # set the weights\n",
    "# optimizer.set_weights(opt_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a model\n",
    "\n",
    "# json_file = open('model.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# model = model_from_json(loaded_model_json)\n",
    "# # load weights into new model\n",
    "\n",
    "model.load_weights(\"model.h5\")\n",
    "\n",
    "# print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(myModel.history['loss'], label='train loss')\n",
    "plt.plot(myModel.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('LossVal_loss')\n",
    "\n",
    "# plot the accuracy\n",
    "plt.plot(myModel.history['accuracy'], label='train acc')\n",
    "plt.plot(myModel.history['val_accuracy'], label='val acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('AccVal_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = []\n",
    "\n",
    "for i in range(len(valid)):\n",
    "    img_dir = 'processed_valid/' + valid.loc[i, 'FILENAME']\n",
    "    image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n",
    "    valid_x.append(image)\n",
    "valid_x = np.array(valid_x).reshape(-1, 256, 64, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(valid_x)\n",
    "decoded = K.get_value(K.ctc_decode(preds, input_length=np.ones(preds.shape[0])*preds.shape[1], greedy=True)[0][0])\n",
    "\n",
    "prediction = []\n",
    "for i in range(len(valid_x)):\n",
    "    prediction.append(num_to_label(decoded[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct characters predicted : 80.40%\n",
      "Correct words predicted      : 67.27%\n"
     ]
    }
   ],
   "source": [
    "y_true = valid.loc[0:len(valid_x), 'IDENTITY']\n",
    "correct_char = 0\n",
    "total_char = 0\n",
    "correct = 0\n",
    "\n",
    "for i in range(len(valid_x)):\n",
    "    pr = prediction[i]\n",
    "    tr = y_true[i]\n",
    "    total_char += len(tr)\n",
    "    \n",
    "    for j in range(min(len(tr), len(pr))):\n",
    "        if tr[j] == pr[j]:\n",
    "            correct_char += 1\n",
    "            \n",
    "    if pr == tr :\n",
    "        correct += 1 \n",
    "    \n",
    "print('Correct characters predicted : %.2f%%' %(correct_char*100/total_char))\n",
    "print('Correct words predicted      : %.2f%%' %(correct*100/len(valid_x)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ae356e75d36f58ae0a0fbe212380b20cef846d46894bb949c9e4d7fe5e6076c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf-gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
